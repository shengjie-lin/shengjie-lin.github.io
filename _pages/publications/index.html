<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/nerfuser.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/nerfuser.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/nerfuser.svg-1400.webp"></source> <img src="/assets/img/publication_preview/nerfuser.svg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="nerfuser.svg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fang2023nerfuser" class="col-sm-8"> <div class="title">NeRFuser: Scalable Scene Representation by NeRF Registration and Blending</div> <div class="author"> Jiading Fang*, <em>Shengjie Lin*</em>, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, and Matthew R. Walter</div> <div class="periodical"> <em>In ICLR Workshop on Neural Fields across Fields</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.13307" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/ripl/nerfuser" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2305.13307"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/statler-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/statler-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/statler-1400.webp"></source> <img src="/assets/img/publication_preview/statler.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="statler.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yoneda2023statler" class="col-sm-8"> <div class="title">Statler: State-Maintaining Language Models for Embodied Reasoning</div> <div class="author"> Takuma Yoneda*, Jiading Fang*, Peng Li*, Huanyu Zhang*, Tianchong Jiang, <em>Shengjie Lin</em>, Ben Picker, David Yunis, Hongyuan Mei, and Matthew R. Walter</div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2306.17840" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://statler-lm.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2306.17840"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/transcrib3d-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/transcrib3d-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/transcrib3d-1400.webp"></source> <img src="/assets/img/publication_preview/transcrib3d.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="transcrib3d.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fang2023transcribe3d" class="col-sm-8"> <div class="title">Transcrib3D: 3D Referring Expression Resolution through Large Language Models</div> <div class="author"> Jiading Fang*, Xiangshan Tan*, <em>Shengjie Lin*</em>, Hongyuan Mei, and Matthew R. Walter</div> <div class="periodical"> <em>In CoRL Workshop on Language and Robot Learning</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> </div> </body></html>